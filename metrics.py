import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os
import sys
import time # D√πng ƒë·ªÉ ƒëo th·ªùi gian ch·∫°y

# --- FIX L·ªñI MODULE NOT FOUND ---
# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c hi·ªán t·∫°i
current_dir = os.path.dirname(os.path.abspath(__file__))
# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c models
models_dir = os.path.join(current_dir, 'models')

# Th√™m folder 'models' v√†o h·ªá th·ªëng t√¨m ki·∫øm c·ªßa Python
if models_dir not in sys.path:
    sys.path.append(models_dir)
# --------------------------------

# Import Model
try:
    from Hybrid import AdaptiveHybridModel
except ImportError:
    # Fallback ƒë·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p file t√™n l√† hybrid.py (th∆∞·ªùng)
    from hybrid import AdaptiveHybridModel

# Th∆∞ m·ª•c l∆∞u bi·ªÉu ƒë·ªì
OUTPUT_DIR = 'static/evaluation_charts'
os.makedirs(OUTPUT_DIR, exist_ok=True)

def run_evaluation():
    print("--- B·∫ÆT ƒê·∫¶U QU√Å TR√åNH ƒê√ÅNH GI√Å (FULL DATASET) ---")
    start_time_total = time.time()
    
    # 1. KH·ªûI T·∫†O MODEL
    data_path = os.path.join(current_dir, 'data', 'processed', 'evaluation')
    print(f">> Data path: {data_path}")
    
    hybrid = AdaptiveHybridModel(data_dir=data_path)
    
    if not hybrid.is_ready:
        print("‚ùå L·ªói: H·ªá th·ªëng ch∆∞a s·∫µn s√†ng. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        return

    # 2. LOAD D·ªÆ LI·ªÜU TEST
    test_file = os.path.join(data_path, 'test_data.csv')
    if not os.path.exists(test_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {test_file}")
        # T·∫°o dummy data n·∫øu kh√¥ng th·∫•y file (ƒë·ªÉ tr√°nh crash)
        test_data = pd.DataFrame({
            'userId': [414, 414, 414, 1, 1],
            'movieId': [1, 2, 3, 1, 5],
            'rating': [5.0, 4.0, 3.0, 4.0, 2.0]
        })
    else:
        test_data = pd.read_csv(test_file)
        # Chu·∫©n h√≥a t√™n c·ªôt
        test_data.rename(columns={'user_id': 'userId', 'movie_id': 'movieId'}, inplace=True)
    
    print(f"‚úÖ ƒê√£ load {len(test_data)} d√≤ng d·ªØ li·ªáu ki·ªÉm th·ª≠.")

    # ==========================================
    # NHI·ªÜM V·ª§ 1: D·ª∞ ƒêO√ÅN L·ªñI (ERROR METRICS - RMSE & MAE)
    # ==========================================
    print("\n[Task 1] ƒêang t√≠nh RMSE v√† MAE (Tr√™n to√†n b·ªô d·ªØ li·ªáu Test)...")
    
    predictions = {'CB': [], 'CF': [], 'Hybrid': [], 'Actual': []}

    # Ch·∫°y loop qua to√†n b·ªô test_data
    for idx, row in test_data.iterrows():
        try:
            # Map t√™n c·ªôt
            u_val = row.get('userId') if 'userId' in row else row.get('user_id')
            m_val = row.get('movieId') if 'movieId' in row else row.get('movie_id')
            
            u = int(u_val)
            m = int(m_val)
            r_true = float(row['rating'])
            
            p_cb = hybrid.cb_model.predict(u, m)
            p_cf = hybrid.cf_model.predict(u, m, k_neighbors=50) 
            p_hybrid = hybrid.predict(u, m) 
            
            predictions['CB'].append(p_cb)
            predictions['CF'].append(p_cf)
            predictions['Hybrid'].append(p_hybrid)
            predictions['Actual'].append(r_true)
        except Exception as e:
            continue 

    results_error = {}
    models_list = ['CB', 'CF', 'Hybrid']
    
    for model in models_list:
        if len(predictions[model]) == 0:
            print(f"‚ö†Ô∏è Model {model} kh√¥ng tr·∫£ v·ªÅ d·ª± ƒëo√°n n√†o.")
            rmse, mae = 0, 0
        else:
            rmse = np.sqrt(mean_squared_error(predictions['Actual'], predictions[model]))
            mae = mean_absolute_error(predictions['Actual'], predictions[model])
        
        results_error[model] = {'RMSE': rmse, 'MAE': mae}
        print(f"   üëâ Model {model}: RMSE = {rmse:.4f}, MAE = {mae:.4f}")

    # ==========================================
    # NHI·ªÜM V·ª§ 2: ƒê√ÅNH GI√Å X·∫æP H·∫†NG (PRECISION & RECALL)
    # ==========================================
    print("\n[Task 2] ƒêang t√≠nh Precision@10 v√† Recall@10...")
    
    k = 10
    precisions = {'CB': [], 'CF': [], 'Hybrid': []}
    recalls = {'CB': [], 'CF': [], 'Hybrid': []}
    
    relevant_data = test_data[test_data['rating'] >= 4.0]
    uid_col = 'userId' if 'userId' in relevant_data.columns else 'user_id'
    mid_col = 'movieId' if 'movieId' in relevant_data.columns else 'movie_id'
    
    # Gom nh√≥m d·ªØ li·ªáu
    test_user_movies = relevant_data.groupby(uid_col)[mid_col].apply(list).to_dict()
    
    # --- THAY ƒê·ªîI: L·∫§Y T·∫§T C·∫¢ USER (FULL DATA) ---
    users_to_test = list(test_user_movies.keys())
    total_users_test = len(users_to_test)
    print(f"‚è≥ ƒêang ch·∫°y ranking cho {total_users_test} users (Vui l√≤ng ƒë·ª£i)...")
    
    for i, u in enumerate(users_to_test):
        # In ti·∫øn ƒë·ªô m·ªói 50 user ƒë·ªÉ bi·∫øt code kh√¥ng b·ªã treo
        if (i + 1) % 50 == 0:
            print(f"   ... ƒêang x·ª≠ l√Ω User th·ª© {i + 1}/{total_users_test}")

        ground_truth = set(test_user_movies[u])
        if len(ground_truth) == 0: continue

        try:
            # L·∫•y danh s√°ch g·ª£i √Ω t·ª´ c√°c model
            recs_cb = hybrid.cb_model.recommend_for_user(u, top_k=k)
            recs_cf = hybrid.cf_model.recommend_for_user(u, top_k=k)
            recs_hybrid = hybrid.recommend_for_user(u, top_k=k)
        except Exception:
            continue
        
        # H√†m t√≠nh Precision v√† Recall
        def calculate_metrics(recs_df, truth_set):
            if recs_df is None or recs_df.empty: return 0.0, 0.0
            col_id = 'movieId' if 'movieId' in recs_df.columns else 'movie_id'
            if col_id not in recs_df.columns: return 0.0, 0.0
            
            rec_ids = set(recs_df[col_id].values)
            hits = len(rec_ids & truth_set)
            prec = hits / k
            rec = hits / len(truth_set) if len(truth_set) > 0 else 0
            return prec, rec

        p, r = calculate_metrics(recs_cb, ground_truth)
        precisions['CB'].append(p); recalls['CB'].append(r)
        
        p, r = calculate_metrics(recs_cf, ground_truth)
        precisions['CF'].append(p); recalls['CF'].append(r)
        
        p, r = calculate_metrics(recs_hybrid, ground_truth)
        precisions['Hybrid'].append(p); recalls['Hybrid'].append(r)

    results_ranking = {}
    for model in models_list:
        avg_p = np.mean(precisions[model]) if precisions[model] else 0
        avg_r = np.mean(recalls[model]) if recalls[model] else 0
        results_ranking[model] = {'Precision@10': avg_p, 'Recall@10': avg_r}
        print(f"   üëâ Model {model}: Precision@10 = {avg_p:.4f}, Recall@10 = {avg_r:.4f}")

    # ==========================================
    # NHI·ªÜM V·ª§ 3: PH√ÇN T√çCH T√çNH TH√çCH NGHI (ALPHA)
    # ==========================================
    print("\n[Task 3] ƒêang ph√¢n t√≠ch Alpha (To√†n b·ªô User trong h·ªá th·ªëng)...")
    user_interactions = []
    alpha_values = []
    
    # --- THAY ƒê·ªîI: L·∫§Y T·∫§T C·∫¢ USER ---
    all_users = list(hybrid.user_manager.user_counts.keys())
    print(f"‚è≥ ƒêang t√≠nh Alpha cho {len(all_users)} users...")
    
    for u in all_users:
        count = hybrid.user_manager.user_counts.get(u, 0)
        alpha = hybrid.calculate_adaptive_weight(u)
        user_interactions.append(count)
        alpha_values.append(alpha)

    # ==========================================
    # NHI·ªÜM V·ª§ 4: V·∫º V√Ä L∆ØU BI·ªÇU ƒê·ªí (VISUALIZATION)
    # ==========================================
    print("\n[Task 4] ƒêang v·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì...")
    sns.set_style("whitegrid")

    def plot_comparison(metric_name, values, title, filename, color_palette, higher_is_better=True):
        plt.figure(figsize=(8, 6))
        ax = sns.barplot(x=models_list, y=values, palette=color_palette, hue=models_list, legend=False)
        
        direction = "Cao h∆°n l√† t·ªët h∆°n" if higher_is_better else "Th·∫•p h∆°n l√† t·ªët h∆°n"
        plt.title(f'{title} ({direction})')
        plt.ylabel(metric_name)
        
        for i, v in enumerate(values):
            ax.text(i, v, f"{v:.4f}", ha='center', va='bottom', fontweight='bold')
        
        save_path = f'{OUTPUT_DIR}/{filename}'
        plt.savefig(save_path)
        plt.close()
        print(f"   Saved: {save_path}")

    # 1. RMSE
    rmse_vals = [results_error[m]['RMSE'] for m in models_list]
    plot_comparison('RMSE', rmse_vals, 'So s√°nh Sai s·ªë RMSE', 
                   'rmse_comparison.png', 'Reds_d', higher_is_better=False)

    # 2. MAE
    mae_vals = [results_error[m]['MAE'] for m in models_list]
    plot_comparison('MAE', mae_vals, 'So s√°nh Sai s·ªë MAE', 
                   'mae_comparison.png', 'Purples_d', higher_is_better=False)

    # 3. Precision
    prec_vals = [results_ranking[m]['Precision@10'] for m in models_list]
    plot_comparison('Precision@10', prec_vals, 'So s√°nh Precision@10', 
                   'precision_comparison.png', 'Greens_d', higher_is_better=True)

    # 4. Recall
    rec_vals = [results_ranking[m]['Recall@10'] for m in models_list]
    plot_comparison('Recall@10', rec_vals, 'So s√°nh Recall@10', 
                   'recall_comparison.png', 'Blues_d', higher_is_better=True)

    # 5. Scatter Plot (Alpha)
    if user_interactions:
        plt.figure(figsize=(10, 6))
        plt.scatter(user_interactions, alpha_values, alpha=0.6, c=alpha_values, cmap='coolwarm')
        plt.colorbar(label='Gi√° tr·ªã Alpha')
        plt.title(f'Alpha th√≠ch nghi (N={len(all_users)} users)')
        plt.xlabel('S·ªë phim ƒë√£ xem (Log Scale)')
        plt.ylabel('Tr·ªçng s·ªë Alpha (Thi√™n v·ªÅ CF)')
        plt.xscale('log')
        plt.savefig(f'{OUTPUT_DIR}/alpha_adaptive_analysis.png')
        plt.close()
        print(f"   Saved: {OUTPUT_DIR}/alpha_adaptive_analysis.png")

    end_time = time.time()
    duration = end_time - start_time_total
    print(f"\n‚úÖ HO√ÄN T·∫§T! T·ªïng th·ªùi gian ch·∫°y: {duration:.2f} gi√¢y.")
    print(f"üìÇ Ki·ªÉm tra bi·ªÉu ƒë·ªì t·∫°i: {os.path.abspath(OUTPUT_DIR)}")

if __name__ == "__main__":
    run_evaluation()